{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIgCNrMfNVME",
        "outputId": "e1de5aec-1f90-45ab-aec0-e3c5b5049536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загружаю: https://en.wikipedia.org/wiki/Cooking\n",
            "Загружаю: https://en.wikipedia.org/wiki/Baking\n",
            "Загружаю: https://en.wikipedia.org/wiki/Frying\n",
            "Загружаю: https://en.wikipedia.org/wiki/Boiling\n",
            "Загружаю: https://en.wikipedia.org/wiki/Simmering\n",
            "Загружено документов: 5\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import sqlite3\n",
        "import re\n",
        "import math\n",
        "from html.parser import HTMLParser\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# статьи Википедии\n",
        "URLS = [\n",
        "    \"https://en.wikipedia.org/wiki/Cooking\",\n",
        "    \"https://en.wikipedia.org/wiki/Baking\",\n",
        "    \"https://en.wikipedia.org/wiki/Frying\",\n",
        "    \"https://en.wikipedia.org/wiki/Boiling\",\n",
        "    \"https://en.wikipedia.org/wiki/Simmering\",\n",
        "]\n",
        "\n",
        "\n",
        "PAGES = {}\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"MiniSearchBot/1.0 (educational project; contact: your_email@example.com)\"\n",
        "}\n",
        "\n",
        "for url in URLS:\n",
        "    print(f\"Загружаю: {url}\")\n",
        "    resp = requests.get(url, headers=headers, timeout=15)\n",
        "    resp.raise_for_status()          # если ошибка HTTP — выбрасываем исключение\n",
        "    PAGES[url] = resp.text\n",
        "\n",
        "    time.sleep(1.0)\n",
        "\n",
        "print(f\"Загружено документов: {len(PAGES)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LinkTextParser(HTMLParser):\n",
        "    \"\"\"\n",
        "    Простой HTML-парсер:\n",
        "    - собирает текст со страницы;\n",
        "    - собирает значения href из ссылок <a>.\n",
        "    Используется для реальных HTML-документов (статей Википедии).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.links = []\n",
        "        self.text_chunks = []\n",
        "\n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        if tag == \"a\":\n",
        "            for name, value in attrs:\n",
        "                if name == \"href\":\n",
        "                    self.links.append(value)\n",
        "\n",
        "    def handle_data(self, data):\n",
        "        text = data.strip()\n",
        "        if text:\n",
        "            self.text_chunks.append(text)\n",
        "\n",
        "def parse_html(html: str):\n",
        "    \"\"\"\n",
        "    Возвращает:\n",
        "    - text: объединённый текст страницы;\n",
        "    - links: исходящие ссылки (сырой список href).\n",
        "    \"\"\"\n",
        "    parser = LinkTextParser()\n",
        "    parser.feed(html)\n",
        "    text = \" \".join(parser.text_chunks)\n",
        "    links = parser.links\n",
        "    return text, links\n",
        "\n",
        "# Стоп-слова: немного русских и английских,\n",
        "# чтобы отфильтровать общие «мусорные» слова.\n",
        "STOPWORDS = {\n",
        "    # Русские\n",
        "    \"и\",\"в\",\"во\",\"на\",\"как\",\"по\",\"о\",\"об\",\"от\",\"за\",\"из\",\"к\",\"ко\",\n",
        "    \"а\",\"но\",\"для\",\"что\",\"это\",\"этот\",\"эта\",\"эти\",\"также\",\"так\",\n",
        "    # Английские\n",
        "    \"the\",\"and\",\"or\",\"for\",\"of\",\"in\",\"on\",\"at\",\"is\",\"are\",\"to\",\"a\",\"an\",\"by\",\"with\",\"as\",\n",
        "    \"this\",\"that\",\"these\",\"those\",\"from\",\"into\",\"about\",\"also\",\"such\",\"more\",\"most\",\n",
        "}\n",
        "\n",
        "def tokenize(text: str):\n",
        "    \"\"\"\n",
        "    Токенизация текста:\n",
        "    - приводим к нижнему регистру;\n",
        "    - выбираем последовательности букв (латиница + кириллица);\n",
        "    - отбрасываем короткие слова и стоп-слова.\n",
        "    \"\"\"\n",
        "    tokens = re.findall(r\"[A-Za-zА-Яа-яЁё]+\", text.lower())\n",
        "    return [t for t in tokens if t not in STOPWORDS and len(t) > 2]\n",
        "\n",
        "# Быстрая проверка парсера на одной реальной странице\n",
        "for url, html in PAGES.items():\n",
        "    text, links = parse_html(html)\n",
        "    print(\"URL:\", url)\n",
        "    print(\"  Текст (фрагмент):\", text[:120].replace(\"\\n\", \" \"), \"...\")\n",
        "    print(\"  Пример ссылок:\", links[:5])\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhdWbwkgNuA6",
        "outputId": "6001fea7-9999-4b75-ebb4-0859d5b82abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://en.wikipedia.org/wiki/Cooking\n",
            "  Текст (фрагмент): Cooking - Wikipedia (function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-langua ...\n",
            "  Пример ссылок: ['#bodyContent', '/wiki/Main_Page', '/wiki/Wikipedia:Contents', '/wiki/Portal:Current_events', '/wiki/Special:Random']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Создание и заполнение базы данных SQLite\n",
        "\n",
        "conn = sqlite3.connect(\":memory:\")\n",
        "cur = conn.cursor()\n",
        "\n",
        "cur.executescript(\"\"\"\n",
        "CREATE TABLE documents(\n",
        "    id      INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    url     TEXT UNIQUE,\n",
        "    title   TEXT,\n",
        "    content TEXT\n",
        ");\n",
        "\n",
        "CREATE TABLE terms(\n",
        "    id      INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    term    TEXT UNIQUE\n",
        ");\n",
        "\n",
        "CREATE TABLE doc_terms(\n",
        "    term_id INTEGER,\n",
        "    doc_id  INTEGER,\n",
        "    tf      INTEGER,\n",
        "    PRIMARY KEY(term_id, doc_id)\n",
        ");\n",
        "\n",
        "CREATE TABLE links(\n",
        "    from_doc INTEGER,\n",
        "    to_doc   INTEGER\n",
        ");\n",
        "\"\"\")\n",
        "\n",
        "for url, html in PAGES.items():\n",
        "    text, links = parse_html(html)\n",
        "    # Заголовок вытащим как первое предложение (фрагмент текста)\n",
        "    title = text.split(\".\")[0][:200]\n",
        "    cur.execute(\n",
        "        \"INSERT INTO documents(url, title, content) VALUES (?, ?, ?)\",\n",
        "        (url, title, text)\n",
        "    )\n",
        "\n",
        "conn.commit()\n",
        "\n",
        "# Карта url -> id\n",
        "cur.execute(\"SELECT id, url FROM documents\")\n",
        "url_to_id = {url: doc_id for doc_id, url in cur.fetchall()}\n",
        "\n",
        "# Подготовим вспомогательную функцию нормализации wiki-ссылок\n",
        "\n",
        "def normalize_wiki_link(href: str):\n",
        "    if not href:\n",
        "        return None\n",
        "    # обрежем якоря и параметры\n",
        "    href = href.split(\"#\")[0].split(\"?\")[0]\n",
        "    href = href.strip()\n",
        "    if not href:\n",
        "        return None\n",
        "\n",
        "    # относительная ссылка вида /wiki/Baking\n",
        "    if href.startswith(\"/wiki/\"):\n",
        "        return \"https://en.wikipedia.org\" + href\n",
        "\n",
        "    # абсолютная ссылка вида https://en.wikipedia.org/wiki/Baking\n",
        "    if href.startswith(\"https://en.wikipedia.org/wiki/\") or href.startswith(\"http://en.wikipedia.org/wiki/\"):\n",
        "        return href\n",
        "\n",
        "    # протокол-независимая ссылка //en.wikipedia.org/wiki/...\n",
        "    if href.startswith(\"//en.wikipedia.org/wiki/\"):\n",
        "        return \"https:\" + href\n",
        "\n",
        "    # всё остальное (внешние сайты и т.п.) нас не интересует\n",
        "    return None\n",
        "\n",
        "# Парсинг, токенизация и заполнение terms / doc_terms / links для реальных страниц\n",
        "for url, html in PAGES.items():\n",
        "    doc_id = url_to_id[url]\n",
        "    text, links = parse_html(html)\n",
        "    tokens = tokenize(text)\n",
        "    counts = Counter(tokens)\n",
        "\n",
        "    # Термины (слова)\n",
        "    for term, tf in counts.items():\n",
        "        cur.execute(\"INSERT OR IGNORE INTO terms(term) VALUES (?)\", (term,))\n",
        "        cur.execute(\"SELECT id FROM terms WHERE term = ?\", (term,))\n",
        "        term_id = cur.fetchone()[0]\n",
        "\n",
        "        cur.execute(\"\"\"\n",
        "            INSERT OR REPLACE INTO doc_terms(term_id, doc_id, tf)\n",
        "            VALUES (?, ?, ?)\n",
        "        \"\"\", (term_id, doc_id, tf))\n",
        "\n",
        "    # Ссылки между пятью статьями\n",
        "    for href in links:\n",
        "        norm = normalize_wiki_link(href)\n",
        "        # Оставляем только ссылки, которые попадают в наш корпус\n",
        "        if norm and norm in url_to_id:\n",
        "            to_id = url_to_id[norm]\n",
        "            cur.execute(\n",
        "                \"INSERT INTO links(from_doc, to_doc) VALUES (?, ?)\",\n",
        "                (doc_id, to_id)\n",
        "            )\n",
        "\n",
        "conn.commit()\n",
        "\n",
        "# Быстрая проверка заполнения\n",
        "cur.execute(\"SELECT COUNT(*) FROM documents\")\n",
        "print(\"Документов:\", cur.fetchone()[0])\n",
        "cur.execute(\"SELECT COUNT(*) FROM terms\")\n",
        "print(\"Терминов:\", cur.fetchone()[0])\n",
        "cur.execute(\"SELECT COUNT(*) FROM doc_terms\")\n",
        "print(\"Записей doc_terms:\", cur.fetchone()[0])\n",
        "cur.execute(\"SELECT from_doc, to_doc FROM links\")\n",
        "print(\"Ссылки (from_doc -> to_doc):\", cur.fetchall())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXy661K4N0mC",
        "outputId": "3fddd6ac-262d-4b0c-9477-7e496eed0ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Документов: 5\n",
            "Терминов: 4901\n",
            "Записей doc_terms: 8811\n",
            "Ссылки (from_doc -> to_doc): [(1, 1), (1, 1), (1, 1), (1, 2), (1, 4), (1, 4), (1, 5), (1, 4), (1, 2), (1, 5), (1, 3), (1, 2), (1, 4), (1, 5), (1, 3), (2, 2), (2, 2), (2, 2), (2, 1), (2, 4), (2, 5), (2, 3), (3, 3), (3, 3), (3, 3), (3, 1), (3, 1), (3, 2), (3, 4), (3, 5), (4, 4), (4, 4), (4, 4), (4, 1), (4, 1), (4, 5), (4, 1), (4, 2), (4, 5), (4, 3), (5, 5), (5, 5), (5, 5), (5, 1), (5, 4), (5, 1), (5, 2), (5, 4), (5, 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Построение графа ссылок для PageRank\n",
        "\n",
        "cur.execute(\"SELECT id FROM documents\")\n",
        "doc_ids = [row[0] for row in cur.fetchall()]\n",
        "N = len(doc_ids)\n",
        "\n",
        "# Список смежности: doc_id -> список doc_id, на которые он ссылается\n",
        "adj = {doc_id: [] for doc_id in doc_ids}\n",
        "\n",
        "cur.execute(\"SELECT from_doc, to_doc FROM links\")\n",
        "for from_doc, to_doc in cur.fetchall():\n",
        "    adj[from_doc].append(to_doc)\n",
        "\n",
        "print(\"Список смежности (adjacency) по реальным статьям:\")\n",
        "for doc_id in sorted(adj):\n",
        "    print(f\"  {doc_id} -> {adj[doc_id]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I8PNqpQN3Ty",
        "outputId": "e158a9b8-01ee-4004-df34-a4524736666a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Список смежности (adjacency) по реальным статьям:\n",
            "  1 -> [1, 1, 1, 2, 4, 4, 5, 4, 2, 5, 3, 2, 4, 5, 3]\n",
            "  2 -> [2, 2, 2, 1, 4, 5, 3]\n",
            "  3 -> [3, 3, 3, 1, 1, 2, 4, 5]\n",
            "  4 -> [4, 4, 4, 1, 1, 5, 1, 2, 5, 3]\n",
            "  5 -> [5, 5, 5, 1, 4, 1, 2, 4, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PageRank через модель MapReduce\n",
        "\n",
        "def pagerank_mapreduce(adj, num_iters=20, damping=0.85):\n",
        "    \"\"\"\n",
        "    adj: словарь doc_id -> список doc_id, на которые ведут исходящие ссылки.\n",
        "\n",
        "    Логика MapReduce:\n",
        "    - \"Map\": каждая страница делит свой текущий PageRank между всеми ссылками;\n",
        "    - \"Reduce\": для каждой страницы суммируем все вклады и применяем формулу PageRank.\n",
        "    \"\"\"\n",
        "    nodes = list(adj.keys())\n",
        "    N = len(nodes)\n",
        "    ranks = {node: 1.0 / N for node in nodes}  # начальное равномерное распределение\n",
        "\n",
        "    for it in range(num_iters):\n",
        "        contributions = []\n",
        "\n",
        "        # Map: генерируем вклады от каждой вершины к соседям\n",
        "        for node in nodes:\n",
        "            out_links = adj[node]\n",
        "            if out_links:\n",
        "                share = ranks[node] / len(out_links)\n",
        "                for dest in out_links:\n",
        "                    contributions.append((dest, share))\n",
        "            else:\n",
        "                # \"висячая\" страница: равномерно раздаём её ранг всем\n",
        "                share = ranks[node] / N\n",
        "                for dest in nodes:\n",
        "                    contributions.append((dest, share))\n",
        "\n",
        "        # Reduce: суммируем вклады по каждой вершине\n",
        "        new_ranks = {node: 0.0 for node in nodes}\n",
        "        for dest, value in contributions:\n",
        "            new_ranks[dest] += value\n",
        "\n",
        "        # Применяем damping-фактор (коэффициент затухания)\n",
        "        for node in nodes:\n",
        "            new_ranks[node] = (1 - damping) / N + damping * new_ranks[node]\n",
        "\n",
        "        ranks = new_ranks\n",
        "\n",
        "    return ranks\n",
        "\n",
        "def print_pagerank(pr_dict, label):\n",
        "    print(label)\n",
        "    for doc_id, pr in sorted(pr_dict.items(), key=lambda x: x[1], reverse=True):\n",
        "        cur.execute(\"SELECT url, title FROM documents WHERE id = ?\", (doc_id,))\n",
        "        url, title = cur.fetchone()\n",
        "        print(f\"  doc_id={doc_id}, PR={pr:.4f}\")\n",
        "        print(f\"     url={url}\")\n",
        "        print(f\"     title={title[:80]}...\")\n",
        "    print()\n",
        "\n",
        "pr_mr = pagerank_mapreduce(adj, num_iters=20)\n",
        "print_pagerank(pr_mr, \"PageRank (MapReduce) для статей:\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuxYKjuzN5_C",
        "outputId": "c8143fde-a642-4592-b747-3df3745dda1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PageRank (MapReduce) для статей:\n",
            "  doc_id=1, PR=0.2199\n",
            "     url=https://en.wikipedia.org/wiki/Cooking\n",
            "     title=Cooking - Wikipedia (function(){var className=\"client-js vector-feature-language...\n",
            "  doc_id=4, PR=0.2143\n",
            "     url=https://en.wikipedia.org/wiki/Boiling\n",
            "     title=Boiling - Wikipedia (function(){var className=\"client-js vector-feature-language...\n",
            "  doc_id=5, PR=0.2028\n",
            "     url=https://en.wikipedia.org/wiki/Simmering\n",
            "     title=Simmering - Wikipedia (function(){var className=\"client-js vector-feature-langua...\n",
            "  doc_id=2, PR=0.1932\n",
            "     url=https://en.wikipedia.org/wiki/Baking\n",
            "     title=Baking - Wikipedia (function(){var className=\"client-js vector-feature-language-...\n",
            "  doc_id=3, PR=0.1699\n",
            "     url=https://en.wikipedia.org/wiki/Frying\n",
            "     title=Frying - Wikipedia (function(){var className=\"client-js vector-feature-language-...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PageRank в стиле Pregel\n",
        "\n",
        "\n",
        "def pagerank_pregel(adj, num_iters=20, damping=0.85):\n",
        "    \"\"\"\n",
        "    Pregel-подход:\n",
        "    - каждая вершина на супер-шаге рассылает свой ранг соседям;\n",
        "    - вершина суммирует все полученные сообщения и пересчитывает PageRank.\n",
        "    \"\"\"\n",
        "    nodes = list(adj.keys())\n",
        "    N = len(nodes)\n",
        "    ranks = {node: 1.0 / N for node in nodes}\n",
        "\n",
        "    for it in range(num_iters):\n",
        "        # Словарь \"сообщений\" для каждой вершины\n",
        "        messages = {node: 0.0 for node in nodes}\n",
        "\n",
        "        # Вершины рассылают свой текущий PageRank\n",
        "        for node in nodes:\n",
        "            out_links = adj[node]\n",
        "            if out_links:\n",
        "                share = ranks[node] / len(out_links)\n",
        "                for dest in out_links:\n",
        "                    messages[dest] += share\n",
        "            else:\n",
        "                # висячая вершина: делим ранг между всеми\n",
        "                share = ranks[node] / N\n",
        "                for dest in nodes:\n",
        "                    messages[dest] += share\n",
        "\n",
        "        # Каждая вершина пересчитывает свой PageRank\n",
        "        for node in nodes:\n",
        "            ranks[node] = (1 - damping) / N + damping * messages[node]\n",
        "\n",
        "    return ranks\n",
        "\n",
        "pr_pregel = pagerank_pregel(adj, num_iters=20)\n",
        "print_pagerank(pr_pregel, \"PageRank (Pregel) для статей:\")\n",
        "\n",
        "print(\"Сравнение MapReduce и Pregel по doc_id:\")\n",
        "for doc_id in sorted(adj.keys()):\n",
        "    print(f\"  doc_id={doc_id}: MR={pr_mr[doc_id]:.4f}, Pregel={pr_pregel[doc_id]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUh7FczQN8RS",
        "outputId": "45f82825-e6ad-4ab7-f09c-92f3f6bb782b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PageRank (Pregel) для статей:\n",
            "  doc_id=1, PR=0.2199\n",
            "     url=https://en.wikipedia.org/wiki/Cooking\n",
            "     title=Cooking - Wikipedia (function(){var className=\"client-js vector-feature-language...\n",
            "  doc_id=4, PR=0.2143\n",
            "     url=https://en.wikipedia.org/wiki/Boiling\n",
            "     title=Boiling - Wikipedia (function(){var className=\"client-js vector-feature-language...\n",
            "  doc_id=5, PR=0.2028\n",
            "     url=https://en.wikipedia.org/wiki/Simmering\n",
            "     title=Simmering - Wikipedia (function(){var className=\"client-js vector-feature-langua...\n",
            "  doc_id=2, PR=0.1932\n",
            "     url=https://en.wikipedia.org/wiki/Baking\n",
            "     title=Baking - Wikipedia (function(){var className=\"client-js vector-feature-language-...\n",
            "  doc_id=3, PR=0.1699\n",
            "     url=https://en.wikipedia.org/wiki/Frying\n",
            "     title=Frying - Wikipedia (function(){var className=\"client-js vector-feature-language-...\n",
            "\n",
            "Сравнение MapReduce и Pregel по doc_id:\n",
            "  doc_id=1: MR=0.2199, Pregel=0.2199\n",
            "  doc_id=2: MR=0.1932, Pregel=0.1932\n",
            "  doc_id=3: MR=0.1699, Pregel=0.1699\n",
            "  doc_id=4: MR=0.2143, Pregel=0.2143\n",
            "  doc_id=5: MR=0.2028, Pregel=0.2028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Построение инвертированного индекса\n",
        "\n",
        "\n",
        "# term_id <-> term\n",
        "cur.execute(\"SELECT id, term FROM terms\")\n",
        "term_id_to_term = {tid: term for tid, term in cur.fetchall()}\n",
        "term_to_id = {term: tid for tid, term in term_id_to_term.items()}\n",
        "\n",
        "# inverted_index: term -> список (doc_id, tf)\n",
        "cur.execute(\"SELECT term_id, doc_id, tf FROM doc_terms\")\n",
        "inverted_index = {}\n",
        "doc_lengths = {doc_id: 0 for doc_id in doc_ids}\n",
        "\n",
        "for term_id, doc_id, tf in cur.fetchall():\n",
        "    term = term_id_to_term[term_id]\n",
        "    inverted_index.setdefault(term, []).append((doc_id, tf))\n",
        "    doc_lengths[doc_id] += tf\n",
        "\n",
        "# сортируем списки по doc_id\n",
        "for term in inverted_index:\n",
        "    inverted_index[term].sort(key=lambda x: x[0])\n",
        "\n",
        "# DF и IDF для TF-IDF-скоринга\n",
        "N = len(doc_ids)\n",
        "df = {term: len(postings) for term, postings in inverted_index.items()}\n",
        "idf = {term: math.log(N / df_t) if df_t > 0 else 0.0\n",
        "       for term, df_t in df.items()}\n",
        "\n",
        "print(\"Пример записей инвертированного индекса (реальные статьи):\")\n",
        "for term, postings in list(inverted_index.items())[:10]:\n",
        "    print(f\"  term='{term}': postings={postings}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXDCngiLN-rJ",
        "outputId": "6b387b45-e5f4-41c0-f2b0-44fd41256d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Пример записей инвертированного индекса (реальные статьи):\n",
            "  term='cooking': postings=[(1, 134), (2, 23), (3, 24), (4, 21), (5, 21)]\n",
            "  term='wikipedia': postings=[(1, 19), (2, 15), (3, 16), (4, 16), (5, 14)]\n",
            "  term='function': postings=[(1, 6), (2, 7), (3, 6), (4, 7), (5, 6)]\n",
            "  term='var': postings=[(1, 9), (2, 7), (3, 9), (4, 7), (5, 7)]\n",
            "  term='classname': postings=[(1, 5), (2, 5), (3, 5), (4, 5), (5, 5)]\n",
            "  term='client': postings=[(1, 3), (2, 3), (3, 3), (4, 3), (5, 3)]\n",
            "  term='vector': postings=[(1, 17), (2, 17), (3, 17), (4, 17), (5, 17)]\n",
            "  term='feature': postings=[(1, 11), (2, 10), (3, 10), (4, 10), (5, 10)]\n",
            "  term='language': postings=[(1, 5), (2, 6), (3, 2), (4, 2), (5, 4)]\n",
            "  term='header': postings=[(1, 7), (2, 3), (3, 3), (4, 3), (5, 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Полнотекстовый поиск: term-at-a-time и document-at-a-time\n",
        "\n",
        "\n",
        "def normalize_query_terms(query: str):\n",
        "    \"\"\"\n",
        "    Нормализация текста запроса:\n",
        "    используем ту же токенизацию и фильтрацию, что и для документов.\n",
        "    \"\"\"\n",
        "    return tokenize(query)\n",
        "\n",
        "def search_taat(query: str, top_k=5):\n",
        "    \"\"\"\n",
        "    Term-at-a-time:\n",
        "    - обрабатываем термины запроса по одному;\n",
        "    - по каждому термину проходим его postings-список и обновляем скор документов.\n",
        "    \"\"\"\n",
        "    terms_q = normalize_query_terms(query)\n",
        "    scores = defaultdict(float)\n",
        "\n",
        "    for term in terms_q:\n",
        "        postings = inverted_index.get(term, [])\n",
        "        w_t = idf.get(term, 0.0)\n",
        "        for doc_id, tf in postings:\n",
        "            scores[doc_id] += tf * w_t  # TF-IDF вклад\n",
        "\n",
        "    # Смешиваем текстовый скор с PageRank\n",
        "    if scores:\n",
        "        max_pr = max(pr_mr.values())\n",
        "        for doc_id in scores:\n",
        "            pr_norm = pr_mr[doc_id] / max_pr\n",
        "            scores[doc_id] = 0.7 * scores[doc_id] + 0.3 * pr_norm\n",
        "\n",
        "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return ranked[:top_k]\n",
        "\n",
        "def search_daat(query: str, top_k=5):\n",
        "    \"\"\"\n",
        "    Document-at-a-time:\n",
        "    - одновременно идём по postings-спискам всех термов;\n",
        "    - обрабатываем документы по одному, собирая вклад всех термов.\n",
        "    \"\"\"\n",
        "    terms_q = normalize_query_terms(query)\n",
        "    postings_lists = [inverted_index.get(t, []) for t in terms_q]\n",
        "    pointers = [0] * len(postings_lists)\n",
        "    scores = {}\n",
        "\n",
        "    if not postings_lists:\n",
        "        return []\n",
        "\n",
        "    max_pr = max(pr_mr.values())\n",
        "\n",
        "    while True:\n",
        "        current_ids = []\n",
        "        for i, lst in enumerate(postings_lists):\n",
        "            if pointers[i] < len(lst):\n",
        "                current_ids.append(lst[pointers[i]][0])\n",
        "\n",
        "        if not current_ids:\n",
        "            break\n",
        "\n",
        "        doc_id = min(current_ids)\n",
        "        score = 0.0\n",
        "\n",
        "        # для текущего doc_id собираем вклад всех термов запроса\n",
        "        for i, lst in enumerate(postings_lists):\n",
        "            if pointers[i] < len(lst) and lst[pointers[i]][0] == doc_id:\n",
        "                tf = lst[pointers[i]][1]\n",
        "                term = terms_q[i]\n",
        "                score += tf * idf.get(term, 0.0)\n",
        "                pointers[i] += 1\n",
        "\n",
        "        pr_norm = pr_mr[doc_id] / max_pr\n",
        "        score = 0.7 * score + 0.3 * pr_norm\n",
        "        scores[doc_id] = score\n",
        "\n",
        "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return ranked[:top_k]\n",
        "\n",
        "def pretty_print_results(results, label):\n",
        "    print(label)\n",
        "    if not results:\n",
        "        print(\"  Нет результатов.\")\n",
        "        return\n",
        "    for rank, (doc_id, score) in enumerate(results, start=1):\n",
        "        cur.execute(\"SELECT url, title FROM documents WHERE id = ?\", (doc_id,))\n",
        "        url, title = cur.fetchone()\n",
        "        print(f\"  {rank}. doc_id={doc_id}, score={score:.4f}, PR={pr_mr[doc_id]:.3f}\")\n",
        "        print(f\"     url={url}\")\n",
        "        print(f\"     title={title[:100]}...\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "B3cn07KEOAhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Демонстрация конвейера поиска\n",
        "\n",
        "\n",
        "# Пример запроса: слова по тематике кулинарии и техник приготовления\n",
        "query = \"cooking baking frying\"\n",
        "\n",
        "results_taat = search_taat(query, top_k=5)\n",
        "results_daat = search_daat(query, top_k=5)\n",
        "\n",
        "pretty_print_results(results_taat, f\"Term-at-a-time (запрос: '{query}')\")\n",
        "pretty_print_results(results_daat, f\"Document-at-a-time (запрос: '{query}')\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxTLJ6GGOCcp",
        "outputId": "6bdbe4c2-bf54-4243-8f1b-1ee5bd8c822e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term-at-a-time (запрос: 'cooking baking frying')\n",
            "  1. doc_id=1, score=0.3000, PR=0.220\n",
            "     url=https://en.wikipedia.org/wiki/Cooking\n",
            "     title=Cooking - Wikipedia (function(){var className=\"client-js vector-feature-language-in-header-enabled v...\n",
            "  2. doc_id=4, score=0.2924, PR=0.214\n",
            "     url=https://en.wikipedia.org/wiki/Boiling\n",
            "     title=Boiling - Wikipedia (function(){var className=\"client-js vector-feature-language-in-header-enabled v...\n",
            "  3. doc_id=5, score=0.2766, PR=0.203\n",
            "     url=https://en.wikipedia.org/wiki/Simmering\n",
            "     title=Simmering - Wikipedia (function(){var className=\"client-js vector-feature-language-in-header-enabled...\n",
            "  4. doc_id=2, score=0.2635, PR=0.193\n",
            "     url=https://en.wikipedia.org/wiki/Baking\n",
            "     title=Baking - Wikipedia (function(){var className=\"client-js vector-feature-language-in-header-enabled ve...\n",
            "  5. doc_id=3, score=0.2318, PR=0.170\n",
            "     url=https://en.wikipedia.org/wiki/Frying\n",
            "     title=Frying - Wikipedia (function(){var className=\"client-js vector-feature-language-in-header-enabled ve...\n",
            "\n",
            "Document-at-a-time (запрос: 'cooking baking frying')\n",
            "  1. doc_id=1, score=0.3000, PR=0.220\n",
            "     url=https://en.wikipedia.org/wiki/Cooking\n",
            "     title=Cooking - Wikipedia (function(){var className=\"client-js vector-feature-language-in-header-enabled v...\n",
            "  2. doc_id=4, score=0.2924, PR=0.214\n",
            "     url=https://en.wikipedia.org/wiki/Boiling\n",
            "     title=Boiling - Wikipedia (function(){var className=\"client-js vector-feature-language-in-header-enabled v...\n",
            "  3. doc_id=5, score=0.2766, PR=0.203\n",
            "     url=https://en.wikipedia.org/wiki/Simmering\n",
            "     title=Simmering - Wikipedia (function(){var className=\"client-js vector-feature-language-in-header-enabled...\n",
            "  4. doc_id=2, score=0.2635, PR=0.193\n",
            "     url=https://en.wikipedia.org/wiki/Baking\n",
            "     title=Baking - Wikipedia (function(){var className=\"client-js vector-feature-language-in-header-enabled ve...\n",
            "  5. doc_id=3, score=0.2318, PR=0.170\n",
            "     url=https://en.wikipedia.org/wiki/Frying\n",
            "     title=Frying - Wikipedia (function(){var className=\"client-js vector-feature-language-in-header-enabled ve...\n",
            "\n"
          ]
        }
      ]
    }
  ]
}